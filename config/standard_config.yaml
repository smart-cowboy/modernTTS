# ARCHITECTURE
decoder_model_dimension: 264
encoder_model_dimension: 256
decoder_num_heads: [4,4,4,4]
encoder_num_heads: [4,4,4,4]
encoder_feed_forward_dimension: 512
decoder_feed_forward_dimension: 512
decoder_prenet_dimension: 256
max_position_encoding: 100000
postnet_conv_filters: 256
postnet_conv_layers: 5
postnet_kernel_size: 5
dropout_rate: 0.1

# LOSSES
stop_loss_scaling: 8

# DATA
n_samples: 100000
n_test: 100
mel_start_value: 4
mel_end_value: -4

# AUDIO
sampling_rate : 22050
n_fft : 2048
mel_channels: 80
hop_length : 275                    # 12.5ms - in line with Tacotron 2 paper
win_length : 1100                   # 50ms - same reason as above
f_min : 40
min_level_db : -100
ref_level_db : 20
peak_norm: False  # if False, only normalizes to peak if peak>1
max_norm: 4 # normalizes values between -max norm, max_norm

# TRAINING
dropout_schedule:
  - [0, 0.5]
learning_rate_schedule:
  - [0, 1.0e-4]
reduction_factor_schedule:
  - [0, 10]
  - [20_000, 5]
  - [50_000, 2]
  - [100_000, 1]
max_steps: 900_000
batch_size: 16
debug: False

# LOGGING
validation_frequency: 500
prediction_frequency: 10_000
weights_save_frequency: 10_000
train_images_plotting_frequency: 1_000
keep_n_weights: 5
keep_checkpoint_every_n_hours: None
n_steps_avg_losses: [100, 500, 1_000, 5_000]
n_predictions: 5
prediction_start_step: 10_000
audio_start_step: 40_000

# TOKENIZER
phoneme_language: 'en'
