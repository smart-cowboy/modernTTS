# ARCHITECTURE
decoder_model_dimension: 264
encoder_model_dimension: 256
decoder_num_heads: [4,4,4,4]
encoder_num_heads: [4,4,4,4]
encoder_feed_forward_dimension: 512
decoder_feed_forward_dimension: 512
decoder_prenet_dimension: 256
encoder_max_position_encoding: 1000
decoder_max_position_encoding: 10000
postnet_conv_filters: 256
postnet_conv_layers: 5
postnet_kernel_size: 5
dropout_rate: 0.1

# FORWARD ARCHITECTURE
encoder_dense_blocks: 1
decoder_dense_blocks: 0
model_dimension: 264

# LOSSES
stop_loss_scaling: 8

# DATA
n_samples: 100000
n_test: 100
mel_start_value: 4
mel_end_value: -4

# AUDIO
sampling_rate: 22050
n_fft: 2048
mel_channels: 80
hop_length: 275                    # 12.5ms - in line with Tacotron 2 paper
win_length: 1100                   # 50ms - same reason as above
f_min: 40
min_level_db: -100
ref_level_db: 20
peak_norm: False  # if False, only normalizes to peak if peak>1
max_norm: 4 # normalizes values between -max norm, max_norm

# TRAINING
dropout_schedule:
  - [0, 0.9]
  - [50_000, 0.]
learning_rate_schedule:
  - [0, 1.0e-4]
head_drop_schedule:
  - [0, 3]
  - [50_000, 1]
reduction_factor_schedule:
  - [0, 10]
  - [20_000, 5]
  - [50_000, 2]
  - [100_000, 1]
max_steps: 900_000
batch_size: 16
debug: False

# LOGGING
validation_frequency: 1_000
prediction_frequency: 1_000
weights_save_frequency: 10_000
train_images_plotting_frequency: 1_000
keep_n_weights: 5
keep_checkpoint_every_n_hours: 12
n_steps_avg_losses: [100, 500, 1_000, 5_000]
n_predictions: 2
prediction_start_step: 1_000
audio_start_step: 5_000
audio_prediction_frequency: 5_000 # converting glim takes time

# TOKENIZER
phoneme_language: 'en'
